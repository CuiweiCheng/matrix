{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Trees and Machine Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Decision trees are tree structures containing rules\n",
    "<li>The leaf nodes of the tree are the \"learned\" categories (or threshold values)\n",
    "<li>A path from the root to a leaf node represents a rule (or a decision path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example: A decision tree with rules on deciding who survived or died on the titanic</h3>\n",
    "<i>Source: https://en.wikipedia.org/wiki/Decision_tree_learning#/media/File:CART_tree_titanic_survivors.png</i>\n",
    "<li>To use the tree, enter a person-data object and you'll get an answer</li>\n",
    "<li>Ex: (\"John Brown\",\"Male\",\"30 years old\", \"3 siblings\") Ans: Survived (89% probability)\n",
    "<li>Ex: (\"Jessica Jones\", \"Female\",\"7 years old\", \"no siblings\") Ans: Survived (73% probability)\n",
    "<li>Ex: (\"Hercules Mulligan\", \"Male\", \"2 years old\", \"20 siblings\") Ans: Died (17% probability)\n",
    "<li>Note that the 17% probability doesn't mean that there is an 83% chance that Mulligan survived!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "image/png": {
       "height": 400,
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"Class+11+-+CART_tree_titanic_survivors.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why decision trees?</h2>\n",
    "<li>Easy to understand \n",
    "<li>Rule finding process is transparent\n",
    "<li>Can handle \"mixed\" categorical(male/female) and numerical (age, number of siblings) data\n",
    "<li>Can handle missing data \n",
    "<li>Can be used to generate partial \"good\" solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why not decision trees?</h2>\n",
    "<li>Finding an optimal tree is a hard problem\n",
    "<li>Overfitting is a HUGE problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Types of decision trees</h2>\n",
    "<ul>\n",
    "<li><b>Classification trees</b>: Uses rules to classify cases into two or more categories (Rocks vs Mines)\n",
    "<ul>\n",
    "<li>Classification trees recursively split the data on a feature value\n",
    "<li>Each split minimizes the entropy (also known as the impurity)\n",
    "<li>Entropy is commonly measured using the GINI cost function (a measure of the probability of misclassification or 'purity')\n",
    "</ul>\n",
    "<li><b>Regression trees</b>: Uses rules to group data into target variable ranges (Wine Quality)\n",
    "<ul>\n",
    "<li>Also split the data on feature values\n",
    "<li>Minimize cost (impurity). Usually the mean squared error\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stopping and Pruning Rules</h3>\n",
    "<li>A minimum count of observations in each leaf node\n",
    "<li>A maximum tree <b>depth</b>\n",
    "<li>A maximum <b>complexity</b> (the number of splits)\n",
    "<li>Using all three, you won't necessarily have a balanced tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predicting wine quality using a decision tree</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "w_df = pd.read_csv(url,header=0,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Examining the dependent variable</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df['quality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Higher dv values indicate a better quality wine\n",
    "<li>Lower dv values indicate a poorer quality wine\n",
    "<li>We'll assume that the values are continuous\n",
    "<li>And use the various features to predict wine quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Build train and test samples</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(w_df, test_size = 0.3)\n",
    "x_train = train.iloc[0:,0:11]\n",
    "y_train = train[['quality']]\n",
    "x_test = test.iloc[0:,0:11]\n",
    "y_test = test[['quality']]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classifiers vs Regressors</h3>\n",
    "<li>Decision tree regressors are used when the target variable is continuous and ordered (wine quality from 0 to 10)\n",
    "<li>Classifiers are used when the target variable is a set of unordered categories (rocks or mines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For wine quality, we need a regressor</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "#model = DecisionTreeRegressor(max_depth = 3)\n",
    "from sklearn import tree\n",
    "model = tree.DecisionTreeRegressor(max_depth=3)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the R-Square for the predicted vs actuals on the text sample\n",
    "print(\"Training R-Square\",model.score(x_train,y_train))\n",
    "print(\"Testing R-Square\",model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>View the tree</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Download and install <a href=\"http://www.graphviz.org/download/\">graphviz</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are having issues using Graphviz in Windows, then try the following steps:\n",
    "<ol>\n",
    "<li>1. Install Graphviz \n",
    "<li>2. After installing graphviz, add it to the Computer's Path. \n",
    "<ul>\n",
    "<li>Go to PC properties \n",
    "<li> Click environment variables in the advanced settings section\n",
    "<li> Add C:\\Program Files (x86)\\Graphviz2.38\\bin\\ to the PATH and click Apply\n",
    "</ul>\n",
    "<li> Install Pydotplus. <b>Note that you will always have to install pydot after graphviz as Pydot is Graphviz's dot language and needs Graphviz for reference</b>. \n",
    "</ol>\n",
    "<h3>Install pydotplus (using pip): Install graphviz before you install pydotplus!</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydotplus --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pydotplus \n",
    "feature_names = [key for key in w_df if not key=='quality']\n",
    "from IPython.display import Image\n",
    "dot_data = tree.export_graphviz(model, out_file=None,feature_names=feature_names)\n",
    "import pydotplus\n",
    "\n",
    "graph = pydotplus.graphviz.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision trees are Entropy minimizers</h3>\n",
    "<li><b>Entropy</b>: a measure of uncertainty in the data<p>\n",
    "<ul>\n",
    "<li>what is the uncertainty in color when you draw a marble from a box of 100 blue marbles?\n",
    "<li>what is the uncertainty when you draw a marble from a box with 50 blue and 50 red marbles?\n",
    "</ul>\n",
    "<li>Entropy minimization: decision tree algorithms seek to partition the data on features in the way that total entropy is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The danger of entropy minimization</h2>\n",
    "<li>In the degenerate case, we can build rules that partition the data into single case subsets\n",
    "<li>The resulting combined entropy will be zero!\n",
    "<li>But the results will be useless because we will likely not be able to predict anything if we get a new case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example</h2>\n",
    "<li>Feature set = {SSN,GENDER,GRE}\n",
    "<li>DV = {GPA}\n",
    "<li>Output rules:\n",
    "<ul>\n",
    "<li>if SSN = x1 then 3.2\n",
    "<li>if SSN = x2 then 4.2\n",
    "<li>One rule for each SSN in our training dataset\n",
    "</ul>\n",
    "<li>This will be totally useless in predicting what a the gpa of a new student is likely to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regression trees</h3>\n",
    "<li>Run regressions for each X to the dependent variable\n",
    "<li>Pick the variable with the most explanatory power and split it at several points\n",
    "<li>Calculate the Mean Square Error of each of the two halves for each split\n",
    "<li>Pick the split point that gives the lowest mse (combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification trees</h1>\n",
    "<h2>Classification trees are used when dealing with categorical dependent variables</h2>\n",
    "<li>Pick a variable and a split point so that the misclassification cost is the lowest.\n",
    "\n",
    "<h3>Rocks and mines data set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "df = pd.read_csv(url,header=None)\n",
    "df[60]=np.where(df[60]=='R',0,1)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.3)\n",
    "x_train = train.iloc[0:,0:60]\n",
    "y_train = train[[60]]\n",
    "x_test = test.iloc[0:,0:60]\n",
    "y_test = test[[60]]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "model = tree.DecisionTreeClassifier(max_depth = 3,criterion='entropy')\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "feature_names = [key for key in df if not key == 60]\n",
    "dot_data = tree.export_graphviz(model, out_file=None,feature_names=feature_names) \n",
    "\n",
    "graph = pydotplus.graphviz.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[tn, fp]\n",
    "#  [fn, tp]]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "p_train=model.predict(x_train)\n",
    "p_test = model.predict(x_test)\n",
    "print(confusion_matrix(p_train,np.array(y_train)))\n",
    "print(confusion_matrix(p_test,np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "(fpr, tpr, thresholds) = roc_curve(y_test,p_test)\n",
    "area = auc(fpr,tpr)\n",
    "pl.clf() #Clear the current figure\n",
    "pl.plot(fpr,tpr,label=\"Out-Sample ROC Curve with area = %1.2f\"%area)\n",
    "\n",
    "pl.plot([0, 1], [0, 1], 'k') #This plots the random (equal probability line)\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('In sample ROC rocks versus mines')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Random forests</h1>\n",
    "<li>Build many decision trees from the data set\n",
    "<li>Let them \"vote\" on how to classify inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ensemble learning random forests</h2>\n",
    "<li>Use a random subset of features and choose the feature to split on from this subset\n",
    "<li>Repeat the process, this gives multiple different trees (the ensemble)\n",
    "<li>The model then predicts y values by letting the trees vote \n",
    "<ul>\n",
    "<li>The forest is given a case\n",
    "<li>Each tree decides which class the case belongs to\n",
    "<li>Votes are tallied\n",
    "<li>The highest vote wins\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "#np.ravel flattens the pandas Series into an np array. That's what the classifier needs\n",
    "model.fit(x_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>\n",
    "<li>The \"score\" function returns the accuracy of the model (percentage correctly classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(x_test)\n",
    "cfm = confusion_matrix(np.ravel(y_test),y_pred)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature importance</h2>\n",
    "<li>Since ensemble methods are picking different features in different trees, they can provide us with an estimate of feature importance\n",
    "<li>For each feature, the model calculates by how much entropy decreases (net across levels) by selecting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "importances = model.feature_importances_\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.barh(range(len(indices)), importances, color='b', align='center')\n",
    "dummy = plt.yticks(range(len(indices)),feature_names)\n",
    "# Sorted\n",
    "#indices = np.argsort(importances)\n",
    "#plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "#dummy = plt.yticks(range(len(indices)),indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding the best ensemble</h3>\n",
    "<li>Using a gridsearch, we can run the random forest classifier on various parameter combinations\n",
    "<li>And then use the classifier with the best accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'n_estimators':(10, 30, 50), #the number of trees\n",
    "     'max_depth':(4,5,6,8,10,15),\n",
    "     'min_samples_split': (2, 4, 8),\n",
    "     'min_samples_leaf': (4,8,12,16)\n",
    "}\n",
    "\n",
    "model = GridSearchCV(RandomForestClassifier(),parameters,cv=3,iid=False)\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "model.best_score_, model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_model = RandomForestClassifier(max_depth=15,min_samples_leaf=12,min_samples_split=4,n_estimators=50)\n",
    "b_model.fit(x_train,np.ravel(y_train))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = b_model.predict(x_test)\n",
    "cfm = confusion_matrix(np.ravel(y_test),y_pred)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bootstrapping / Bagging</h2>\n",
    "<li>Create a synthetic dataset by drawing sample cases \"with replacement\" from the training set\n",
    "<li>Run the decision tree algorithm on this dataset\n",
    "<li>Repeat on multiple such synthetic datasets\n",
    "<li>Let the many trees vote on the class for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "model=BaggingClassifier()\n",
    "model.fit(x_train,np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'n_estimators':(30, 50), #the number of trees\n",
    "     'max_samples':(30,40,50),\n",
    "     'max_features':(5,10,20),  \n",
    "}\n",
    "\n",
    "model = GridSearchCV(BaggingClassifier(),parameters,cv=3,iid=False)\n",
    "model.fit(x_train, np.ravel(y_train))\n",
    "model.best_score_, model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BaggingClassifier(max_features=5,max_samples=10,n_estimators=30)\n",
    "model.fit(x_train,np.ravel(y_train))\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why random forests?</h2>\n",
    "<li>Can deal with missing data\n",
    "<ul>\n",
    "<li>If a feature is missing from a case, that case is discarded when a feature is considered for inclusion in the tree\n",
    "<li>In ensemble learning, since we're working with subsets of the features, every feature will figure in some tree or the other (and will have a vote)\n",
    "</ul>\n",
    "<li>Useful when the data available is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Why not random forests?</h2>\n",
    "<li>Danger of overfitting, especially when the sample is small or when the number of classes is small\n",
    "<li>Can't explain the results. The rules are opaque since many trees are voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
