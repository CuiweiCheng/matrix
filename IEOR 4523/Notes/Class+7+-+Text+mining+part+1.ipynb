{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preparation</h1>\n",
    "<li>Download the nltk corpus (see below)\n",
    "<li>Download Class 7 - Data.zip from canvas and unzip it in a local folder in your directory\n",
    "<li>From the data folder copy the files: 2013-Obama.txt and data/2017-Trump.txt to:\n",
    "<ul>\n",
    "<li>~/nltk_data/corpora/inaugural/ (~ indicates your home directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOU NEED TO RUN THIS ONLY ONCE!\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working with text!</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>nltk: Python's natural language toolkit</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ntlk documentation link:</h3> http://www.nltk.org/api/nltk.html\n",
    "<h3>Commands cheat sheet</h3> https://blogs.princeton.edu/etc/files/2014/03/Text-Analysis-with-NLTK-Cheatsheet.pdf\n",
    "<h3>nltk book</h3>http://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Types of analysis</h2>\n",
    "<li>Sentiment analysis: Deciding whether a document (or concept) is positive or negative\n",
    "<li>Entity analysis: Identifying entities (Named entities, Parts of speech) and properties of these entities\n",
    "<li>Topic analysis: Deciding what the major topics associated with a piece of text\n",
    "<li>Text summarization: Summarizing a document (Cliff notes version!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment Analysis</h2>\n",
    "Identify entities and emotions in a sentence and use these to determine if the entity is being viewed positively or negatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Easy examples</h3>\n",
    "<li>I had an <b style=\"color:green\">excellent</b> souffle at the restaurant Cavity Maker</li>\n",
    "<li>Excellent is a positive word for both the souffle as well as for the restaurant</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Not so easy examples</h3>\n",
    "<h4>Often, looking at words alone is not enough to figure out the sentiment</h4>\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for a ‘stuck at home’ snow day</i></li> This one is easy since it includes an explicit positive opinion using a positive word\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for using as a liner for your cat’s litter box</i></li> Not so simple! The positive word \"excellent\" is used with a negative connotation. \n",
    "<li><i>The Girl on the Train is <span style=\"color:green\">better</span> than Gone Girl</i></li> The positive word is used as a comparator. Whether the writer likes The Girl on the Train or not depends on what he or she thinks of Gone Girl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bottom line</h4>\n",
    "Sentiment analysis is generally a starting point in analyzing a text and is then coupled with other techniques (e.g., topic analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentiment analysis is usually done using a corpus of positive and negative words</h2>\n",
    "<li>Some sources compile lists of positive and negative words\n",
    "<li>Others include the polarity - the degree of positivity or negativity - of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sources of sentiment coded words</h2>\n",
    "<ol>\n",
    "<li>Hu and Liu's sentiment analysis lexicon: words coded as either positive or negative</li>\n",
    "<ul>\n",
    "<li>http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "<li>http://ptrckprry.com/course/ssd/data/negative-words.txt\n",
    "</ul>\n",
    "<li>NRC Emotion Lexicon: words coded into emotional categories (many languages)</li>\n",
    "<ul>\n",
    "<li>http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</li>\n",
    "</ul>\n",
    "<li>SentiWordNet: Lists of words weighted by positive or negative sentiment. Includes guidance on how to use the words</li>\n",
    "<ul>\n",
    "<li>http://sentiwordnet.isti.cnr.it/</li>\n",
    "</ul>\n",
    "<li>Vader Sentiment tool: 7800 words with positive or negative polarity</li>\n",
    "<ul>\n",
    "<li>Included with python nltk</li>\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Our examples</h2>\n",
    "<li>Compiled set of reviews of neighborhood restaurants\n",
    "<li>Presidential inaugural addresses (from Washington to Trump)\n",
    "<li>Some data from yelp using the yelp API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Simple sentiment analysis</h3>\n",
    "Compute the proportion of positive and negative words in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_neg_words():\n",
    "    def get_words(url):\n",
    "        import requests\n",
    "        words = requests.get(url).content.decode('latin-1')\n",
    "        word_list = words.split('\\n')\n",
    "        index = 0\n",
    "        while index < len(word_list):\n",
    "            word = word_list[index]\n",
    "            if ';' in word or not word:\n",
    "                word_list.pop(index)\n",
    "            else:\n",
    "                index+=1\n",
    "        return word_list\n",
    "\n",
    "    #Get lists of positive and negative words\n",
    "    p_url = 'http://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "    n_url = 'http://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "    positive_words = get_words(p_url)\n",
    "    negative_words = get_words(n_url)\n",
    "    return positive_words,negative_words\n",
    "\n",
    "positive_words,negative_words = get_pos_neg_words()\n",
    "\n",
    "p = set(positive_words)\n",
    "n = set(negative_words)\n",
    "\n",
    "print(p.intersection(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read the text being analyzed and count the proportion of positive and negative words in the text</h4>\n",
    "<li>We'll look at the reviews of two restaurants in the Morningside Heights neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Class 7 - Data/community.txt','r') as f:\n",
    "    community = f.read()\n",
    "with open('./Class 7 - Data/le_monde.txt','r') as f:\n",
    "    le_monde = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Compute sentiment by looking at the proportion of positive and negative words in the text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "cpos = cneg = lpos = lneg = 0\n",
    "for word in word_tokenize(community):\n",
    "    if word in positive_words:\n",
    "        cpos+=1\n",
    "    if word in negative_words:\n",
    "        cneg+=1\n",
    "for word in word_tokenize(le_monde):\n",
    "    if word in positive_words:\n",
    "        lpos+=1\n",
    "    if word in negative_words:\n",
    "        lneg+=1\n",
    "print(\"community {0:1.2f}%\\t {1:1.2f}%\\t {2:1.2f}%\".format(cpos/len(word_tokenize(community))*100,\n",
    "                                                        cneg/len(word_tokenize(community))*100,\n",
    "                                                        (cpos-cneg)/len(word_tokenize(community))*100))\n",
    "print(\"le monde  {0:1.2f}%\\t {1:1.2f}%\\t {2:1.2f}%\".format(lpos/len(word_tokenize(le_monde))*100,\n",
    "                                                        lneg/len(word_tokenize(le_monde))*100,\n",
    "                                                        (lpos-lneg)/len(word_tokenize(le_monde))*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's functionalize this</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pos_neg_sentiment_analysis(text_list,debug=False):\n",
    "    positive_words,negative_words = get_pos_neg_words()\n",
    "    from nltk import word_tokenize\n",
    "    results = list()\n",
    "    for text in text_list:\n",
    "        cpos = cneg = 0\n",
    "        for word in word_tokenize(text[1]):\n",
    "            if word in positive_words:\n",
    "                if debug:\n",
    "                    print(\"Positive\",word)\n",
    "                cpos+=1\n",
    "            if word in negative_words:\n",
    "                if debug:\n",
    "                    print(\"Negative\",word)\n",
    "                cneg+=1\n",
    "        results.append((text[0],cpos/len(word_tokenize(text[1])),cneg/len(word_tokenize(text[1]))))\n",
    "    return results\n",
    "\n",
    "do_pos_neg_sentiment_analysis([('community',community),('le_monde',le_monde)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple sentiment analysis using NRC data</h2>\n",
    "<li>NRC data codifies words with emotions</li>\n",
    "<li>14,182 words are coded into 2 sentiments and 8 emotions</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For example, the word abandonment is associated with anger, fear, sadness and has a negative sentiment</h4>\n",
    "<li>abandoned\tanger\t1\n",
    "<li>abandoned\tanticipation\t0\n",
    "<li>abandoned\tdisgust\t0\n",
    "<li>abandoned\tfear\t1\n",
    "<li>abandoned\tjoy\t0\n",
    "<li>abandoned\tnegative\t1\n",
    "<li>abandoned\tpositive\t0\n",
    "<li>abandoned\tsadness\t1\n",
    "<li>abandoned\tsurprise\t0\n",
    "<li>abandoned\ttrust\t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Read the NRC sentiment data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrc = \"./Class 7 - Data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "count=0\n",
    "emotion_dict=dict()\n",
    "with open(nrc,'r') as f:\n",
    "    all_lines = list()\n",
    "    for line in f:\n",
    "        if count < 46:\n",
    "            count+=1\n",
    "            continue\n",
    "        line = line.strip().split('\\t')\n",
    "        if int(line[2]) == 1:\n",
    "            if emotion_dict.get(line[0]):\n",
    "                emotion_dict[line[0]].append(line[1])\n",
    "            else:\n",
    "                emotion_dict[line[0]] = [line[1]]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Functionalize this</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nrc_data():\n",
    "    nrc = \"./Class 7 - Data/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "    count=0\n",
    "    emotion_dict=dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count+=1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = get_nrc_data()\n",
    "emotion_dict['abandoned']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Yelp API</h1>\n",
    "<li>https://www.yelp.com/developers/documentation/v3\n",
    "<li>log into yelp (top right hand corner of the page)\n",
    "<li>Click <span style=\"color:blue\">Create App</span> on the left hand menu bar\n",
    "<li>Enter app info (leave optional stuff blank)\n",
    "<li>Copy the client id and client secret to a secure place (this notebook should do the trick or use a text file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./YelpAPIKeys.txt','r') as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        if count == 0:\n",
    "            CLIENT_ID = line.strip()\n",
    "        if count == 1:\n",
    "            API_KEY = line.strip()\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLIENT_ID,API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API constants, you shouldn't have to change these.\n",
    "API_HOST = 'https://api.yelp.com' #The API url header\n",
    "SEARCH_PATH = '/v3/businesses/search' #The path for an API request to find businesses\n",
    "BUSINESS_PATH = '/v3/businesses/'  # The path to get data for a single business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now we can get reviews</h3>\n",
    "<li>get_reviews(location,number=15) returns the reviews of \"number\" (default=15) restaurants in the vicinity of \"location\"\n",
    "<li>First, we'll write a function that gets  restaurants in the vicinity of location\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restaurants(api_key,location,number=15):\n",
    "    import requests\n",
    "    \n",
    "    #First we get the access token\n",
    "    #Set up the search data dictionary\n",
    "    search_data = {\n",
    "    'term': \"restaurant\",\n",
    "    'location': location.replace(' ', '+'),\n",
    "    'limit': number\n",
    "    }\n",
    "    url = API_HOST + SEARCH_PATH\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "    response = requests.request('GET', url, headers=headers, params=search_data).json()\n",
    "    businesses = response.get('businesses')\n",
    "    return businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_restaurants(API_KEY,\"Columbia University, New York, NY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Then a function that, given a business id, returns a string containing the reviews</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_review(api_key,business_id):\n",
    "    import json\n",
    "    import requests\n",
    "    business_path = BUSINESS_PATH + business_id+\"/reviews\"\n",
    "    url = API_HOST + business_path\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "\n",
    "\n",
    "    response = requests.request('GET', url, headers=headers).json()\n",
    "   \n",
    "    review_text = ''\n",
    "    for review in response['reviews']:\n",
    "        review_text += review['text']\n",
    "    return review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_business_review(API_KEY,'flat-top-new-york')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Finally, put all this together to get review data for the set of restaurants</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(location,number=15):\n",
    "\n",
    "    restaurants = get_restaurants(API_KEY,location,number)\n",
    "\n",
    "    if not restaurants:\n",
    "        return None\n",
    "    review_list = list()\n",
    "    for restaurant in restaurants:\n",
    "        restaurant_name = restaurant['name']\n",
    "        restaurant_id = restaurant['id']\n",
    "        review_text = get_business_review(API_KEY,restaurant_id)\n",
    "        \n",
    "        review_list.append((restaurant_name,review_text))\n",
    "    return review_list\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snippets = get_reviews(\"Columbia University, New York, NY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A function that analyzes emotions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_analyzer(text,emotion_dict=emotion_dict):\n",
    "    #Set up the result dictionary\n",
    "    emotions = {x for y in emotion_dict.values() for x in y} \n",
    "    #list comprehension - emotion for (emotion_list in emotion_dict.values() for emotion in emotion_list\n",
    "    emotion_count = dict()\n",
    "    for emotion in emotions:\n",
    "        emotion_count[emotion] = 0\n",
    "\n",
    "    #Analyze the text and normalize by total number of words\n",
    "    total_words = len(text.split())\n",
    "    for word in text.split():\n",
    "        if emotion_dict.get(word):\n",
    "            for emotion in emotion_dict.get(word):\n",
    "                emotion_count[emotion] += 1/total_words\n",
    "    return emotion_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we can analyze the emotional content of the review snippets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %1s\\t%1s %1s %1s %1s   %1s %1s %1s %1s\"%(\n",
    "        \"restaurant\",\"fear\",\"trust\",\"negative\",\"positive\",\"joy\",\"disgust\",\"anticip\",\n",
    "        \"sadness\",\"surprise\"))\n",
    "        \n",
    "for snippet in all_snippets:\n",
    "    text = snippet[1]\n",
    "    result = emotion_analyzer(text)\n",
    "    print(\"%-12s %1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\"%(\n",
    "        snippet[0][0:10],result['fear'],result['trust'],\n",
    "          result['negative'],result['positive'],result['joy'],result['disgust'],\n",
    "          result['anticipation'],result['sadness'],result['surprise']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's functionalize this</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>For easy of analysis, we'll do the following:</h3>\n",
    "<li>Generalize it so that we can analyze any document type, not just restaurant reviews\n",
    "<li>Output a dataframe containing the results. This will make analyzing of the results easier\n",
    "<li>We'll decide whether or not we should print the output from the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparative_emotion_analyzer(text_tuples,object_name=\"Restaurant\",print_output=False):\n",
    "    if print_output:\n",
    "        print(\"%-20s %1s\\t%1s %1s %1s %1s   %1s %1s %1s %1s\"%(object_name,\n",
    "                                                              \"fear\",\"trust\",\"negative\",\"positive\",\n",
    "                                                              \"joy\",\"disgust\",\"anticip\", \"sadness\",\n",
    "                                                              \"surprise\"))\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(columns=[object_name,'Fear','Trust','Negative',\n",
    "                           'Positive','Joy','Disgust','Anticipation',\n",
    "                           'Sadness','Surprise'],)\n",
    "    df.set_index(object_name,inplace=True)\n",
    "    \n",
    "    output = df    \n",
    "    for text_tuple in text_tuples:\n",
    "        text = text_tuple[1] \n",
    "        result = emotion_analyzer(text)\n",
    "        if print_output:\n",
    "            print(\"%-20s %1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\\t%1.2f\"%(\n",
    "                text_tuple[1][0:20],result['fear'],result['trust'],\n",
    "                  result['negative'],result['positive'],result['joy'],result['disgust'],\n",
    "                  result['anticipation'],result['sadness'],result['surprise']))\n",
    "        df.loc[text_tuple[0]] = [result['fear'],result['trust'],\n",
    "                  result['negative'],result['positive'],result['joy'],result['disgust'],\n",
    "                  result['anticipation'],result['sadness'],result['surprise']]\n",
    "    return output\n",
    "#And test it        \n",
    "comparative_emotion_analyzer(all_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Package the emotion analyzer with the yelp API to get a yelp data analyzer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_nearby_restaurants(address,number=15):\n",
    "    snippets = get_reviews(address,number)\n",
    "    comparative_emotion_analyzer(snippets)\n",
    "\n",
    "#And test it    \n",
    "analyze_nearby_restaurants(\"Columbia University\",15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test it on some other place\n",
    "analyze_nearby_restaurants(\"221 Baker Street, London, UK\",15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Working with organized bodies of texts</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text corpora</h2>\n",
    "<li>Corpus: An organized set of text documents</li>\n",
    "<li>Examples:\n",
    "<ol>\n",
    "<li>The collection of inaugural speeches\n",
    "<li>An entire book\n",
    "<li>Collection of all books by Graham Greene\n",
    "<li>Collection of tweets by Donald Trump\n",
    "<li>Collection of tweets that reference AAPL\n",
    "</ol>\n",
    "<li>As the examples illustrate, the texts in a corpus are related and what a corpora contains depends on what sort of analysis you want to do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building text corpora</h1>\n",
    "<li>Read a collection of documents from a directory\n",
    "<li>Use APIs to get text documents or fragments\n",
    "<ul>Examples:\n",
    "<li>Tweets\n",
    "<li>Yelp reviews\n",
    "</ul>\n",
    "<li>Use existing corpora\n",
    "<ul>\n",
    "<li>nltk.download() downloads sample corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating a corpus from text files</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's do a detailed comparison of local restaurants</h2>\n",
    "<h4>I've saved a few reviews for each restaurant in four directories</h4>\n",
    "<h4>We'll use the PlainTextCorpusReader to read these directories</h4>\n",
    "<li>PlainTextCorpusReader reads all matching files in a directory and saves them by file-ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Structure example ###\n",
    "# For community:\n",
    "# - Root folder - \"Class 7 - Data/community\"\n",
    "# - file names - \"community.*\"\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "restaurants = ['community', 'le_monde', 'shakeshack', 'fiveguys']\n",
    "restaurants_data = {}\n",
    "for restaurant in restaurants:\n",
    "    restaurants_data[restaurant] = PlaintextCorpusReader('Class 7 - Data/%s' % restaurant, '%s.*' % restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_data['shakeshack'].fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_data['shakeshack'].raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We need to construct text tuples that match the format of the argument of comparative_emotion_analyzer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_tuples = []\n",
    "for key in restaurants_data.keys():\n",
    "    restaurant_tuples.append((key, restaurants_data[key].raw()))\n",
    "\n",
    "comparative_emotion_analyzer(restaurant_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using nltk sample corpora</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>nltk contains a large corpora of pre-tokenized text</h2>\n",
    "Load it using the command:<p>\n",
    "<b>You should have already done this!</b><br>\n",
    "nltk.download()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Import the corpora</h4>\n",
    "<li>Look for texts under nltk_data in your home directory</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Often, a comparitive analysis helps us understand text better</h1>\n",
    "<h2>Let's look at US Presidential Inaugural speeches</h2>\n",
    "<h4>Copy the files 2013-Obama.txt and 2017-Trump.txt to the nltk_data/corpora/inaugural directory. nltk_data should be under your home directory</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.raw('1861-Lincoln.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_addresses = list()\n",
    "for file in inaugural.fileids():\n",
    "    all_addresses.append((file,inaugural.raw(file)))\n",
    "all_addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's compare the speeches by emotion using our function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speeches = comparative_emotion_analyzer(all_addresses,print_output=False,object_name=\"President\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Try the following</h2>\n",
    "<li>The most positive presidential speeches\n",
    "<li>The most negative presidential speeches\n",
    "<li>The most \"surprise\" oriented presidential speeches\n",
    "<li>The most Net Positive speeches:\n",
    "<ul>\n",
    "<li>Positive = Trust + Positive\t+ Joy + Anticipation\n",
    "<li>Negative = Fear + Negative + Disgust + Sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speeches.sort_values(by=\"Surprise\",ascending=False)\n",
    "all_speeches[\"All_Pos\"]=(all_speeches['Trust']+all_speeches['Positive']+ all_speeches['Joy']+ all_speeches['Anticipation'])\n",
    "all_speeches[\"All_Neg\"]=(all_speeches['Fear']+all_speeches['Negative']+ all_speeches['Disgust']+ all_speeches['Sadness'])\n",
    "all_speeches['Net']=all_speeches[\"All_Pos\"]-all_speeches[\"All_Neg\"]\n",
    "all_speeches.sort_values(by=\"Net\",ascending=False)['Net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Naive sentiment analysis on inaugural speeches</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = do_pos_neg_sentiment_analysis([(x[0],x[1]) for x in all_addresses])\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sents,key=lambda x: x[1]-x[2],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Twitter APIs</h3>\n",
    "<li>Sign in to the developer site using your twitter account\n",
    "<li>Go to https://apps.twitter.com/app/new \n",
    "<li>Give your app a name and a description\n",
    "<li>Enter anything for website (e.g., http://www.columbia.edu)\n",
    "<li>Leave callback url blank\n",
    "<li>Accept the terms and conditions and create an account\n",
    "<li>Then, click on the \"Keys and Access Tokens\" tab\n",
    "<li>Copy the two consumer keys to a text file\n",
    "<li>Scroll down and click \"Generate Access key and Secret\"\n",
    "<li>Copy the two access keys to the text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./TwitterAPIKeys.txt\",'r') as token_file:\n",
    "    contents = token_file.read().split('\\n')\n",
    "    consumer_key = contents[0]\n",
    "    consumer_secret = contents[1]\n",
    "    access_token = contents[2]\n",
    "    access_token_secret = contents[3]\n",
    "    \n",
    "print(consumer_key,consumer_secret,access_token,access_token_secret,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>tweepy</h2>\n",
    "<li>A python library that interfaces with the twitter API\n",
    "<li>Returns a lot of useful stuff but we'll only look at the tweet text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Set up an authentication object using OAuth and send a search request</h4>\n",
    "<li>Tweepy constructs a \"list like\" SearchResults object\n",
    "<li>Each item in SearchResults is a Status object\n",
    "<li>The status object has a _json attribute that contains a json tweet\n",
    "<li>http://docs.tweepy.org/en/v3.5.0/getting_started.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "search_term = 'AAPL'\n",
    "## fill in your search query and store your results in a variable\n",
    "results = api.search(q = search_term, lang = \"en\", result_type = \"recent\", count = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(results))\n",
    "print(len(results))\n",
    "print(type(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0]._json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0]._json['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>We can save this to files that can then be read by a plaintextcorpus reader\n",
    "<li>The order is important\n",
    "<li>We could use  datetime for ordering\n",
    "<li>But we'll just number them for now\n",
    "<li>Saving to a file also helps us build a tweet corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'AAPL'\n",
    "for i in range(len(results)):\n",
    "    fname = search_term+'.'+str(len(results)-i)\n",
    "    with open('./Class 7 - Data/tweets/'+fname,'w') as f:\n",
    "        f.write(results[i]._json['text']+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Now we can do sentiment analysis on these tweets</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "tweets_root = \"./Class 7 - Data/tweets\"\n",
    "apple_files = \"AAPL.*\"\n",
    "apple_data = PlaintextCorpusReader(tweets_root,apple_files)\n",
    "\n",
    "apple_data.raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_pos_neg_sentiment_analysis([['apple',apple_data.raw()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_emotion_analyzer([['apple',apple_data.raw()]],object_name='Apple Tweets',print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Let's package this</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_term,consumer_key=consumer_key,consumer_secret=consumer_secret,\n",
    "               access_token=access_token,access_token_secret=access_token_secret):\n",
    "    import tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    results = api.search(q = search_term, lang = \"en\", result_type = \"recent\", count = 1000)\n",
    "    for i in range(len(results)):\n",
    "        fname = search_term+'.'+str(len(results)-i)\n",
    "        with open('./Class 7 - Data/tweets/'+fname,'w') as f:\n",
    "            f.write(results[i]._json['text']+'\\n')\n",
    "\n",
    "get_tweets(search_term=\"GOOG\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>And do a comparative analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "tweets_root = \"./Class 7 - Data/tweets\"\n",
    "apple_files = \"AAPL.*\"\n",
    "google_files = \"GOOG.*\"\n",
    "apple_data = PlaintextCorpusReader(tweets_root,apple_files)\n",
    "google_data = PlaintextCorpusReader(tweets_root,google_files)\n",
    "do_pos_neg_sentiment_analysis([['apple',apple_data.raw()],['google',google_data.raw()]])\n",
    "comparative_emotion_analyzer([\n",
    "    ['apple',apple_data.raw()],\n",
    "    ['google',google_data.raw()]],\n",
    "    object_name='Equity',print_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Simple analysis: Word Clouds</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's see what sort of words the snippets use</h4>\n",
    "<li>First we'll combine all snippets into one string\n",
    "<li>Then we'll generate a word cloud using the words in the string\n",
    "<li>You may need to install wordcloud using pip\n",
    "<li>pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=''\n",
    "for snippet in all_snippets:\n",
    "    text+=snippet[1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white').generate(text)\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>word cloud comparison</h2>\n",
    "<li>We'll remove short words and look only at words longer than 6 letters\n",
    "<li>And then do a side by side comparison of the word clouds for our four restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = restaurant_tuples\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Remove unwanted words\n",
    "#As we look at the cloud, we can get rid of words that don't make sense by adding them to this variable\n",
    "DELETE_WORDS = []\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "#Remove short words\n",
    "MIN_LENGTH = 5\n",
    "def remove_short_words(text_string,min_length = MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1)\n",
    "    return text_string\n",
    "\n",
    "\n",
    "#Set up side by side clouds\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 2\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))\n",
    "\n",
    "for i in range(0,len(texts)):\n",
    "    text_string = remove_words(texts[i][1])\n",
    "    text_string = remove_short_words(text_string)\n",
    "    ax = axes[i//2, i%2] \n",
    "    ax.set_title(texts[i][0])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000,max_words=20).generate(text_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [('trump',inaugural.raw('2017-Trump.txt')),('Obama',inaugural.raw('2013-Obama.txt')),\n",
    "         ('Bush',inaugural.raw('2001-Bush.txt')),('Clinton',inaugural.raw('1997-Clinton.txt'))]\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Remove unwanted words\n",
    "#As we look at the cloud, we can get rid of words that don't make sense by adding them to this variable\n",
    "DELETE_WORDS = []\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "#Remove short words\n",
    "MIN_LENGTH = 5\n",
    "def remove_short_words(text_string,min_length = MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1)\n",
    "    return text_string\n",
    "\n",
    "\n",
    "#Set up side by side clouds\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 2\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))\n",
    "\n",
    "for i in range(0,len(texts)):\n",
    "    text_string = remove_words(texts[i][1])\n",
    "    text_string = remove_short_words(text_string)\n",
    "    ax = axes[i//2, i%2] \n",
    "    ax.set_title(texts[i][0])\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000,max_words=20).generate(text_string)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Simple Analysis: Complexity</h1>\n",
    "<h4>We'll look at four complexity factors</h4>\n",
    "<li>average word length: longer words adds to complexity\n",
    "<li>average sentence length: longer sentences are more complex (unless the text is rambling!)\n",
    "<li>vocabulary: the ratio of unique words used to the total number of words (more variety, more complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>token:</b> A sequence (or group) of characters of interest. For e.g., in the below analysis, a token = a word\n",
    "<li>Generally: A token is the base unit of analysis</li>\n",
    "<li>So, the first step is to convert text into tokens and nltk text object</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct tokens (words/sentences) from the text\n",
    "text = restaurants_data['le_monde'].raw()\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize \n",
    "sentences = nltk.Text(sent_tokenize(text))\n",
    "print(len(sentences))\n",
    "words = nltk.Text(word_tokenize(text))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars=len(text)\n",
    "num_words=len(word_tokenize(text))\n",
    "num_sentences=len(sent_tokenize(text))\n",
    "vocab = {x.lower() for x in word_tokenize(text)}\n",
    "print(num_chars,int(num_chars/num_words),int(num_words/num_sentences),(len(vocab)/num_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Functionalize this</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complexity(text):\n",
    "    num_chars=len(text)\n",
    "    num_words=len(word_tokenize(text))\n",
    "    num_sentences=len(sent_tokenize(text))\n",
    "    vocab = {x.lower() for x in word_tokenize(text)}\n",
    "    return len(vocab),int(num_chars/num_words),int(num_words/num_sentences),len(vocab)/num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_complexity(restaurant_data['le_monde'].raw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in restaurant_tuples:\n",
    "    (vocab,word_size,sent_size,vocab_to_text) = get_complexity(text[1])\n",
    "    print(\"{0:15s}\\t{1:1.2f}\\t{2:1.2f}\\t{3:1.2f}\\t{4:1.2f}\".format(text[0],vocab,word_size,sent_size,vocab_to_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Comparing complexity of restaurant reviews won't get us anything useful</h3>\n",
    "<h3>Let's look at something more useful</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's look at the complexity of the speeches by four presidents</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural_texts = [('trump',inaugural.raw('2017-Trump.txt')),\n",
    "         ('obama',inaugural.raw('2013-Obama.txt')),\n",
    "         ('jackson',inaugural.raw('1829-Jackson.txt')),\n",
    "         ('washington',inaugural.raw('1789-Washington.txt'))]\n",
    "for text in inaugural_texts:\n",
    "    (vocab,word_size,sent_size,vocab_to_text) = get_complexity(text[1])\n",
    "    print(\"{0:15s}\\t{1:1.2f}\\t{2:1.2f}\\t{3:1.2f}\\t{4:1.2f}\".format(text[0],vocab,word_size,sent_size,vocab_to_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analysis over time</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The files are arranged over time so we can analyze how complexity has changed between Washington and Trump</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "sentence_lengths = list()\n",
    "for fileid in inaugural.fileids():\n",
    "    sentence_lengths.append(get_complexity(' '.join(inaugural.words(fileid)))[2])\n",
    "plt.plot(sentence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>dispersion plots</h1>\n",
    "<h2>Dispersion plots show the relative frequency of words over the text</h2>\n",
    "<h3>Let's see how the frequency of some words has changed over the course of the republic</h3>\n",
    "<h3>That should give us some idea of how the focus of the nation has changed</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"government\", \"citizen\", \"freedom\", \"duties\", \"America\",'independence','God','patriotism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>You can use dispersion plots to identify important characters in a book</h4>\n",
    "<li>The main characters in Sense and Sensibility are Elinor, Marianne, Edward, and Willoughby\n",
    "<li>Who are the more important, the men or the women?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.dispersion_plot(['Elinor','Marianne','Edward','Willoughby'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Of the characters in the book, which are likely major and which minor?</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.dispersion_plot(['Elinor','Marianne','Edward','Willoughby','Brandon','Fanny'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stemming</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We may want to use word stems rather than the part of speect form</h4>\n",
    "<li>For example: patriot, patriotic, patriotism all express roughly the same idea\n",
    "<li>nltk has a stemmer that implements the \"Porter Stemming Algorithm\" (https://tartarus.org/martin/PorterStemmer/)\n",
    "<li>We'll push everything to lowercase as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "text = inaugural.raw()\n",
    "striptext = text.replace('\\n\\n', ' ')\n",
    "striptext = striptext.replace('\\n', ' ')\n",
    "sentences = sent_tokenize(striptext)\n",
    "words = word_tokenize(striptext)\n",
    "text = nltk.Text([p_stemmer.stem(i).lower() for i in words])\n",
    "text.dispersion_plot([\"govern\", \"citizen\", \"free\", \"america\",'independ','god','patriot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Weighted sentiment analysis using Vader</h2>\n",
    "<h4>Vader contains a list of 7500 features weighted by how positive or negative they are</h4>\n",
    "<h4>It uses these features to calculate stats on how positive, negative and neutral a passage is</h4>\n",
    "<h4>And combines these results to give a compound sentiment (higher = more positive) for the passage</h4>\n",
    "<h4>Human trained on twitter data and generally considered good for informal communication</h4>\n",
    "<h4>10 humans rated each feature in each tweet in context from -4 to +4</h4>\n",
    "<h4>Calculates the sentiment in a sentence using word order analysis</h4>\n",
    "<li>\"marginally good\" will get a lower positive score than \"extremely good\"\n",
    "<h4>Computes a \"compound\" score based on heuristics (between -1 and +1)</h4>\n",
    "<h4>Includes sentiment of emoticons, punctuation, and other 'social media' lexicon elements</h4>\n",
    "    <h4>For more see http://datameetsmedia.com/vader-sentiment-analysis-explained/</h4> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['pos','neg','neu','compound']\n",
    "texts = restaurant_tuples\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for i in range(len(texts)):\n",
    "    name = texts[i][0]\n",
    "    sentences = sent_tokenize(texts[i][1])\n",
    "    pos=compound=neu=neg=0\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        \n",
    "        pos+=vs['pos']/(len(sentences))\n",
    "        neu+=vs['neu']/(len(sentences))\n",
    "        neg+=vs['neg']/(len(sentences))\n",
    "        compound+=vs['compound']/(len(sentences))\n",
    "    print(name,pos,neg,neu,compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>And functionalize this as well</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_comparison(texts):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    headers = ['pos','neg','neu','compound']\n",
    "    print(\"Name\\t\",'  pos\\t','neg\\t','neu\\t','compound')\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for i in range(len(texts)):\n",
    "        name = texts[i][0]\n",
    "        sentences = sent_tokenize(texts[i][1])\n",
    "        pos=compound=neu=neg=0\n",
    "        for sentence in sentences:\n",
    "            vs = analyzer.polarity_scores(sentence)\n",
    "            \n",
    "            pos+=vs['pos']/(len(sentences))\n",
    "            neu+=vs['neu']/(len(sentences))\n",
    "            neg+=vs['neg']/(len(sentences))\n",
    "            compound+=vs['compound']/(len(sentences))\n",
    "        print('%-10s'%name,'%1.2f\\t'%pos,'%1.2f\\t'%neg,'%1.2f\\t'%neu,'%1.2f\\t'%compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(restaurant_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(inaugural_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Named Entity Detection</h1>\n",
    "<h4>People, places, organizations</h4>\n",
    "Named entities are often the subject of sentiments so identifying them can be very useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Named entity detection is based on Part-of-speech tagging of words and chunks (groups of words)</h4>\n",
    "<li>Start with sentences (using a sentence tokenizer)\n",
    "<li>tokenize words in each sentence\n",
    "<li>chunk them. ne_chunk identifies likely chunked candidates (ne = named entity)\n",
    "<li>Finally build chunks using nltk's guess on what members of chunk represent (people, place, organization)\n",
    "<li>English pickle in the 'punkt' package contains english grammar information\n",
    "<li>We can load it and then use it to parse a sentence into constituent sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sample_text = \"\"\"\n",
    "I was walking along thinking of many things. For e.g., I walked with my friend Bilkees Bijou through the campus of Columbia University. I \n",
    "thought of birds, of bees, of sealing wax. I thought of cabbages and kings.\n",
    "\"\"\"\n",
    "sent_detector.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>word_tokenize collects the words from a sentence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = nltk.word_tokenize(sent_detector.tokenize(sample_text)[1])\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>pos_tag tags the word with nltk's best guess as to the part of speech</h4>\n",
    "<li>nltk uses Penn Treebank tagging</li>\n",
    "<li>https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ne_chunk creates a \"Sentence Tree\" of parts of speech using a tokenized list of words</h4>\n",
    "<li>words that are candidate entities have an attribute \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(word_list)\n",
    "chunked = nltk.ne_chunk(tagged)\n",
    "chunked\n",
    "#chunked[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We're going to use hasattr() to select items from the Tree. </h4>\n",
    "hasattr() is a python function that checks whether a name/string is an attribute of an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_class(object):\n",
    "    def __init__(self,x):\n",
    "        name = x\n",
    "    def check(self):\n",
    "        return self.name\n",
    "\n",
    "y = my_class('Jack')\n",
    "hasattr(y,'check')\n",
    "# dir(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in chunked:\n",
    "    try:\n",
    "        print(j,j.label())\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(word_list)\n",
    "chunked = nltk.ne_chunk(tagged)\n",
    "hasattr(chunked[-2],'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en={}\n",
    "try:\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(sample_text)\n",
    "    for sentence in sentences:\n",
    "            tokenized = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            chunked = nltk.ne_chunk(tagged)\n",
    "            for tree in chunked:\n",
    "                if hasattr(tree, 'label'):\n",
    "                    ne = ' '.join(c[0] for c in tree.leaves())\n",
    "                    # print(ne)\n",
    "                    en[ne] = [tree.label(), ' '.join(c[1] for c in tree.leaves())]\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "#print(en)\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can now do this on our actual text. Let's try with community_data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en={}\n",
    "try:\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(restaurants_data['community'].raw().strip())\n",
    "    for sentence in sentences:\n",
    "            tokenized = nltk.word_tokenize(sentence)\n",
    "            tagged = nltk.pos_tag(tokenized)\n",
    "            chunked = nltk.ne_chunk(tagged)\n",
    "            for tree in chunked:\n",
    "                if hasattr(tree, 'label'):\n",
    "                    ne = ' '.join(c[0] for c in tree.leaves())\n",
    "                    en[ne] = [tree.label(), ' '.join(c[1] for c in tree.leaves())]\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>And functionalize this</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_text(text,label_type='ALL'):\n",
    "    en={}\n",
    "    try:\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sentences = sent_detector.tokenize(text.strip())\n",
    "        for sentence in sentences:\n",
    "                tokenized = nltk.word_tokenize(sentence)\n",
    "                tagged = nltk.pos_tag(tokenized)\n",
    "                chunked = nltk.ne_chunk(tagged)\n",
    "                for tree in chunked:\n",
    "                    if hasattr(tree, 'label'):\n",
    "                        if not label_type == \"ALL\":\n",
    "                            if not tree.label() == label_type:\n",
    "                                continue\n",
    "                        ne = ' '.join(c[0] for c in tree.leaves())\n",
    "                        en[ne] = [tree.label(), ' '.join(c[1] for c in tree.leaves())]\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "    return en\n",
    "get_labeled_text(restaurants_data['community'].raw(),'ORGANIZATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Assuming we've done a good job of identifying named entities, we can get an affect score on entities</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example for the word service\n",
    "meaningful_sents = list()\n",
    "i=0\n",
    "for sentence in sentences:\n",
    "    if 'service' in sentence:\n",
    "        i+=1\n",
    "        meaningful_sents.append((i,sentence))\n",
    "\n",
    "vader_comparison(meaningful_sents)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We could also develop a affect calculator for common terms in our domain (e.g., food items)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affect(text,word,lower=True):\n",
    "    import nltk\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = sent_detector.tokenize(text.strip())\n",
    "    sentence_count = 0\n",
    "    running_total = 0\n",
    "    for sentence in sentences:\n",
    "        if lower: \n",
    "            sentence = sentence.lower()\n",
    "            word = word.lower()\n",
    "        if word in sentence:\n",
    "            vs = analyzer.polarity_scores(sentence) \n",
    "            running_total += vs['compound']\n",
    "            sentence_count += 1\n",
    "    if sentence_count == 0: return 0\n",
    "    return running_total/sentence_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>And compare different texts on the affect score of the terms</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_affect(restaurants_data['community'].raw(),'service',True))\n",
    "print(get_affect(restaurants_data['le_monde'].raw(),'service',True))\n",
    "print(get_affect(restaurants_data['shakeshack'].raw(),'service',True))\n",
    "print(get_affect(restaurants_data['fiveguys'].raw(),'service',True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Or look for the \"good\" and \"bad\" characters in a piecce of text</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_affect(gutenberg.raw('shakespeare-hamlet.txt'),'Gertrude',False))\n",
    "print(get_affect(gutenberg.raw('shakespeare-hamlet.txt'),'Hamlet',False))\n",
    "print(get_affect(gutenberg.raw('shakespeare-hamlet.txt'),'Horatio',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>We can apply this to any text</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_labeled_text(inaugural.raw('2017-trump.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in get_labeled_text(inaugural.raw('2009-obama.txt'),'PERSON'):\n",
    "    print(key,get_affect(inaugural.raw('2009-obama.txt'),key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in get_labeled_text(inaugural.raw('2017-trump.txt'),'PERSON'):\n",
    "    print(key,get_affect(inaugural.raw('2017-trump.txt'),key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>The nltk function concordance prints text fragments around a word</h4>\n",
    "<li>Useful for a quick look but it \"prints\" not \"returns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text(restaurants_data['community'].words()).concordance('Columbia',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.Text(nltk.word_tokenize(inaugural.raw('2009-obama.txt'))).concordance('Sahn',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
